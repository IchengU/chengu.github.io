<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[selenium+phantomjs抓取动态网页数据]]></title>
    <url>%2F2016%2F07%2F18%2Fselenium-phantomjs%E6%8A%93%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[接到fang88的技术考核题，要求如下: 使用python3 只写一个crawl.py文件，传递url参数 url示例：https://www.lennar.com/new-homes/washington/seattlehttps://www.lennar.com/new-homes/texas/houston 爬取后的数据ajax抛给指定的接口。 因为经常会爬一些数据，所以大体看了一下，就一个网址，感觉应该挺简单的。考虑到平时都用py2的scrapy来做爬虫，今天正好再熟悉下py3，也没有用bs4，因为最近发现lxml的xpath解析挺爽的直接代码开撸，嘿嘿！ 首先，根据要求，需要拿到参数，命令行如下:python3 crawl.py https://www.lennar.com/new-homes/washington/seattle1234if len(argv) != 2: print(&apos;参数格式错误&apos;)else: print(argv[1]) 开始分析网址、抓取数据，分析网页内容，发现都是动态数据，因此选择用selenium和phantomjs来抓取12345678910driver = webdriver.PhantomJS(executable_path=&apos;/Users/apple/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs&apos;)driver.get(url)selector = etree.HTML(driver.page_source.encode(&apos;utf-8&apos;))items = selector.xpath(&apos;//div[@class=&quot;comm-item clearfix&quot;]&apos;) for item in items: temp = item.xpath(&apos;./h1/a/text()&apos;) name = temp[0] if len(temp) &gt; 0 else &apos;&apos; temp = item.xpath(&apos;./h2/a/text()&apos;) name = name + &apos; &apos; + (temp[0] if len(temp) &gt; 0 else &apos;&apos;) print(name) 通过上述代码，很快就抓到首页要的内容，接下来准备抓分页，发现是#标签，然后用chrome来network检查元素，查看post的接口，找到相关的，但post的参数太多，所以想用之前做模拟登陆时的模拟点击来实现，代码如下：1driver.find_element_by_xpath(&apos;//div[@class=&quot;sptop&quot;]//a[@class=&quot;ir next&quot;]&apos;).click() 运行调试，报错了：ElementNotVisibleException通过报错，然后按以往的经验，加上延时，发现仍然无效，不明所以之后，尝试其他链接的模拟点击，发现可以。最后选择最原始也是最有效的调用js方法来解决。(如果有哪位高人知道原因，可以告诉我，非常感谢)12with open(&apos;test.html&apos;, &apos;w+&apos;) as f: f.write(driver.page_source) 保存下网页，去查看相关的js及变量的内容。通过检查发现调用了两个接口，先做第一个接口，分析参数为页面里js的变量，于是通过js来得到:12acetParams = driver.execute_script(&quot;return facetContextJSON.params&quot;)pageState = driver.execute_script(&quot;return pageState&quot;) 接着通过postman来抓取的header，复制进去，并通过xpath抓取到总页数，通过requests来获取json数据12345def get_otherpage(page, facetParams, pageState, headers): pageState[&apos;pn&apos;] = page payload = json.dumps(&#123;&apos;searchState&apos;:facetParams, &apos;pageState&apos;: pageState&#125;) url = &apos;https://cn.lennar.com/Services/REST/Facets.svc/GetFacetResults&apos; response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers) 发现成功获取json结果，接着分析第二个接口，参数是第一个接口的返回部分数据，执行代码如下：123456789url = &quot;https://cn.lennar.com/Services/Rest/SearchMethods.svc/GetCommunityDetails&quot;pageState.update(&#123;&quot;pt&quot;:&quot;C&quot;,&quot;ic&quot;:19,&quot;ss&quot;:0,&quot;attr&quot;:&quot;No ne&quot;,&quot;ius&quot;:False&#125;)payload = json.dumps(&#123;&apos;facetResults&apos;: response.json()[&apos;fr&apos;], &apos;pageState&apos;:pageState&#125;)response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers)for item in response.json(): name = item[&apos;cnm&apos;] if item[&apos;mcm&apos;]: name += item[&apos;mcm&apos;] print(name) 到此，所有需求的数据都可以得到了。因为只是测试，所以没有封装成类，也直接使用了postman抓到的cookie.测试代码在github]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac系统下，用excel打开csv文件，中文乱码问题]]></title>
    <url>%2F2014%2F03%2F25%2Fmac%E7%B3%BB%E7%BB%9F%E4%B8%8B%EF%BC%8C%E7%94%A8excel%E6%89%93%E5%BC%80csv%E6%96%87%E4%BB%B6%EF%BC%8C%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[csv中文乱码问题 今天要导出在mongo的数据，直接输入命令mongoexport -d test -c testitem --type=csv -f name,phone,tel,address -o ./test.csv 导出后直接用excel打开，发现中文全是乱码。于是google，发现用excel导入文本的方式也出错(可能是我的文件里本身数据有一些格式问题)、使用sublime另存utf-8格式也是不行。很是纠结，正准备自己写个脚本来完成的时候，找到了问题的解决方法：命令行输入：iconv -f UTF8 -t GB18030 test.csv &gt;test_new.csv发现好使。 另外一种方式：直接将mac电脑上导出的test.csv发到windows上，用wps打开也是正常的，另存为xls格式，再发回mac上，用excel打开也是正常的了。 特做此备忘遇坑问题记录！！！]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>遇坑记录</tag>
      </tags>
  </entry>
</search>