<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Oracle linux下部署分布式redis、分布式mongo]]></title>
    <url>%2F2017%2F08%2F30%2FOracle-linux%E4%B8%8B%E9%83%A8%E7%BD%B2%E5%88%86%E5%B8%83%E5%BC%8Fredis%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8Fmongo%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[将Centos的yum源更换为国内的阿里云源]]></title>
    <url>%2F2017%2F07%2F31%2F%E5%B0%86Centos%E7%9A%84yum%E6%BA%90%E6%9B%B4%E6%8D%A2%E4%B8%BA%E5%9B%BD%E5%86%85%E7%9A%84%E9%98%BF%E9%87%8C%E4%BA%91%E6%BA%90%2F</url>
    <content type="text"><![CDATA[有的时候yum安装一些软件，直接404或者timeout，此时可以更改为国内的源。阿里云Linux安装软件镜像源阿里云是最近新出的一个镜像源。得益与阿里云的高速发展，这么大的需求，肯定会推出自己的镜像源。阿里云Linux安装镜像源地址：http://mirrors.aliyun.com/CentOS系统更换软件安装源第一步：备份你的原镜像文件，以免出错后可以恢复。mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 第二步：下载新的CentOS-Base.repo 到/etc/yum.repos.d/CentOS 6wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoCentOS 7wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo第三步：运行yum makecache生成缓存yum clean allyum makecache]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>问题汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch学习(一)]]></title>
    <url>%2F2017%2F07%2F27%2FElasticsearch%E5%AD%A6%E4%B9%A0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[对于全文检索，只是在刚毕业时研究过sphinx的皮毛，当时项目中选用了sphinx的中文版coreseek，感觉真心强大。而在后来的工作中，也没有再去深入的研究过，而之后看到的文章中，提到最多的只有Solr、Lucene、Elasticsearch这三个基于java的全文检索库，今天工作之余，好好研究一番。 先做了下术语调查，发现Solr与Lucene现在都是Apache基金在维护，Lucene只是一个java框架，如果想使用的话必须要使用java，在程序中集成，了解其原理，所以要求相对较高了。而Solr是最流行的企业级搜索引擎，Solr4 还增加了NoSQL支持，它成熟稳定，还支持html、pdf、doc、xls、json、xml、csv等多种格式的索引。但其缺点也相对明显，数据量增大、搜索效率会降低，实时搜索会阻塞io，效率不高。Elasticsearch是一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎，可以说Lucene是当今最先进，最高效的全功能开源搜索引擎框架，使用它做全文搜索时，只需要使用统一开发好的API即可，而不需要了解其背后复杂的Lucene的运行原理。它在全文搜索、实时搜索、分布式搜索、无缝扩展机器处理pb级别数据等方面都非常出色。因此，这里我选择研究Elasticsearch，并记录下自己的所有理解。 安装首先从官网下载安装包：https://www.elastic.co/downloads/elasticsearch，这里我下载的是zip包，版本5.5.1。接下来解压文件unzip elasticsearch-5.5.1.zip 接下来安装marvel插件，marvel一个管理和监控的工具，是个插件： 12cd elasticsearch-5.5.1./bin/plugin -i elasticsearch/marvel/latest 此时，如果想关闭监控，则可以使用以下命令：echo &#39;marvel.agent.enabled: false&#39; &gt;&gt; ./config/elasticsearch.yml 运行./bin/elasticsearch此时如果只有本地访问，则可以修改配置文件 elasticsearch.yml中network.host(注意配置文件格式不是以#开头的要空一格，：后要空一格) 为network.host: 0.0.0.0如果想在后台以守护进程模式运行，添加-d参数。 测试执行下面的测试命令即可以看到结果curl &#39;http://localhost:9200/?pretty&#39;也可以使用下面命令来关闭它：curl -XPOST &#39;http://localhost:9200/_shutdown&#39; 查看marvel和sense如果你安装了Marvel（作为管理和监控的工具），就可以在浏览器里通过以下地址访问它：http://localhost:9200/_plugin/marvel/你可以在Marvel中通过点击dashboards，在下拉菜单中访问Sense开发者控制台，或者直接访问以下地址：http://localhost:9200/_plugin/marvel/sense/ 端口及通信java客户端分为节点客户端(只加入集群、无数据存储)、传输客户端(不加入集群，只请求至节点客户端)，两种客户端与集群的交互端口均为9300，假设不开此端口，则不会组 成集群，9200端口则是其他语言通过restful api的形式与Elasticsearch进行通信。 使用Elasticsearch以json结构的文档存储。Elasticsearch集群可以包含多个索引(indices)（数据库），每一个索引可以包含多个类型(types)（表），每一个类型包含多个文档(documents)（行），然后每个文档包含多个字段(Fields)（列）。倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。与传统关系型数据库的类比图12Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields 简单例子如下：12345678PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 其中megacorp：索引名，employee：类型名；1：这个员工的ID。(另外，再put一次，则更新此文档) 检索获取的一条数据，保存在_source节点里GET /megacorp/employee/1 简单搜索，默认搜索结果10条，结果保存在hits数组节点里。GET /megacorp/employee/_search 查询搜索，结果也在hits数组节点里GET /megacorp/employee/_search?q=last_name:Smith 使用DSL(Domain Specific Language特定领域语言)语句查询12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125; 带过滤器的搜索1234567891011121314151617GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;filtered&quot; : &#123; &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125; &lt;1&gt; &#125; &#125;, &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;smith&quot; &lt;2&gt; &#125; &#125; &#125; &#125;&#125; 搜索短语：如果match_phrase改为match，则会搜索出包含rock或者climbing的结果12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125; 搜索结果高亮：12345678910111213GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;about&quot; : &#123;&#125; &#125; &#125;&#125; 聚合，就是完成数据的统计分析，类似于sql的group by12345678GET /megacorp/employee/_search&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125; 返回结果如下：12345678910111213141516171819202122&#123; ... &quot;hits&quot;: &#123; ... &#125;, &quot;aggregations&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; 加上搜索语句的聚合123456789101112131415GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;last_name&quot;: &quot;smith&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125; 返回结果如下：12345678910111213...&quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ]&#125; 聚合也允许分级汇总。例如，让我们统计每种兴趣下职员的平均年龄：12345678910111213GET /megacorp/employee/_search&#123; &quot;aggs&quot; : &#123; &quot;all_interests&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;interests&quot; &#125;, &quot;aggs&quot; : &#123; &quot;avg_age&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回结果如下：1234567891011121314151617181920212223242526...&quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2, &quot;avg_age&quot;: &#123; &quot;value&quot;: 28.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1, &quot;avg_age&quot;: &#123; &quot;value&quot;: 35 &#125; &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1, &quot;avg_age&quot;: &#123; &quot;value&quot;: 25 &#125; &#125; ]&#125;]]></content>
      <categories>
        <category>全文检索</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selenium+phantomjs抓取动态网页数据]]></title>
    <url>%2F2016%2F07%2F18%2Fselenium-phantomjs%E6%8A%93%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[接到fang88的技术考核题，要求如下: 使用python3 只写一个crawl.py文件，传递url参数 url示例：https://www.lennar.com/new-homes/washington/seattlehttps://www.lennar.com/new-homes/texas/houston 爬取后的数据ajax抛给指定的接口。 因为经常会爬一些数据，所以大体看了一下，就一个网址，感觉应该挺简单的。考虑到平时都用py2的scrapy来做爬虫，今天正好再熟悉下py3，也没有用bs4，因为最近发现lxml的xpath解析挺爽的直接代码开撸，嘿嘿！ 首先，根据要求，需要拿到参数，命令行如下:python3 crawl.py https://www.lennar.com/new-homes/washington/seattle1234if len(argv) != 2: print(&apos;参数格式错误&apos;)else: print(argv[1]) 开始分析网址、抓取数据，分析网页内容，发现都是动态数据，因此选择用selenium和phantomjs来抓取12345678910driver = webdriver.PhantomJS(executable_path=&apos;/Users/apple/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs&apos;)driver.get(url)selector = etree.HTML(driver.page_source.encode(&apos;utf-8&apos;))items = selector.xpath(&apos;//div[@class=&quot;comm-item clearfix&quot;]&apos;) for item in items: temp = item.xpath(&apos;./h1/a/text()&apos;) name = temp[0] if len(temp) &gt; 0 else &apos;&apos; temp = item.xpath(&apos;./h2/a/text()&apos;) name = name + &apos; &apos; + (temp[0] if len(temp) &gt; 0 else &apos;&apos;) print(name) 通过上述代码，很快就抓到首页要的内容，接下来准备抓分页，发现是#标签，然后用chrome来network检查元素，查看post的接口，找到相关的，但post的参数太多，所以想用之前做模拟登陆时的模拟点击来实现，代码如下：1driver.find_element_by_xpath(&apos;//div[@class=&quot;sptop&quot;]//a[@class=&quot;ir next&quot;]&apos;).click() 运行调试，报错了：ElementNotVisibleException通过报错，然后按以往的经验，加上延时，发现仍然无效，不明所以之后，尝试其他链接的模拟点击，发现可以。最后选择最原始也是最有效的调用js方法来解决。(如果有哪位高人知道原因，可以告诉我，非常感谢)12with open(&apos;test.html&apos;, &apos;w+&apos;) as f: f.write(driver.page_source) 保存下网页，去查看相关的js及变量的内容。通过检查发现调用了两个接口，先做第一个接口，分析参数为页面里js的变量，于是通过js来得到:12acetParams = driver.execute_script(&quot;return facetContextJSON.params&quot;)pageState = driver.execute_script(&quot;return pageState&quot;) 接着通过postman来抓取的header，复制进去，并通过xpath抓取到总页数，通过requests来获取json数据12345def get_otherpage(page, facetParams, pageState, headers): pageState[&apos;pn&apos;] = page payload = json.dumps(&#123;&apos;searchState&apos;:facetParams, &apos;pageState&apos;: pageState&#125;) url = &apos;https://cn.lennar.com/Services/REST/Facets.svc/GetFacetResults&apos; response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers) 发现成功获取json结果，接着分析第二个接口，参数是第一个接口的返回部分数据，执行代码如下：123456789url = &quot;https://cn.lennar.com/Services/Rest/SearchMethods.svc/GetCommunityDetails&quot;pageState.update(&#123;&quot;pt&quot;:&quot;C&quot;,&quot;ic&quot;:19,&quot;ss&quot;:0,&quot;attr&quot;:&quot;No ne&quot;,&quot;ius&quot;:False&#125;)payload = json.dumps(&#123;&apos;facetResults&apos;: response.json()[&apos;fr&apos;], &apos;pageState&apos;:pageState&#125;)response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers)for item in response.json(): name = item[&apos;cnm&apos;] if item[&apos;mcm&apos;]: name += item[&apos;mcm&apos;] print(name) 到此，所有需求的数据都可以得到了。因为只是测试，所以没有封装成类，也直接使用了postman抓到的cookie.测试代码在github]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac系统下，用excel打开csv文件，中文乱码问题]]></title>
    <url>%2F2014%2F03%2F25%2Fmac%E7%B3%BB%E7%BB%9F%E4%B8%8B%EF%BC%8C%E7%94%A8excel%E6%89%93%E5%BC%80csv%E6%96%87%E4%BB%B6%EF%BC%8C%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[csv中文乱码问题 今天要导出在mongo的数据，直接输入命令mongoexport -d test -c testitem --type=csv -f name,phone,tel,address -o ./test.csv 导出后直接用excel打开，发现中文全是乱码。于是google，发现用excel导入文本的方式也出错(可能是我的文件里本身数据有一些格式问题)、使用sublime另存utf-8格式也是不行。很是纠结，正准备自己写个脚本来完成的时候，找到了问题的解决方法：命令行输入：iconv -f UTF8 -t GB18030 test.csv &gt;test_new.csv发现好使。 另外一种方式：直接将mac电脑上导出的test.csv发到windows上，用wps打开也是正常的，另存为xls格式，再发回mac上，用excel打开也是正常的了。 特做此备忘遇坑问题记录！！！]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>遇坑记录</tag>
      </tags>
  </entry>
</search>