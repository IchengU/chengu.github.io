<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rottens&#39;s blog</title>
  <subtitle>努力什么时候都不会晚，加油！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/chengu/chengu.github.io/"/>
  <updated>2017-07-18T11:10:56.000Z</updated>
  <id>https://github.com/chengu/chengu.github.io/</id>
  
  <author>
    <name>Rottens</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>selenium+phantomjs抓取动态网页数据</title>
    <link href="https://github.com/chengu/chengu.github.io/2016/07/18/selenium-phantomjs%E6%8A%93%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE/"/>
    <id>https://github.com/chengu/chengu.github.io/2016/07/18/selenium-phantomjs抓取动态网页数据/</id>
    <published>2016-07-18T08:01:01.000Z</published>
    <updated>2017-07-18T11:10:56.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>接到fang88的技术考核题，要求如下</strong>:</p>
<ol>
<li>使用python3</li>
<li>只写一个crawl.py文件，传递url参数<blockquote>
<p><strong>url示例</strong>：<br><a href="https://www.lennar.com/new-homes/washington/seattle" target="_blank" rel="external">https://www.lennar.com/new-homes/washington/seattle</a><br><a href="https://www.lennar.com/new-homes/texas/houston" target="_blank" rel="external">https://www.lennar.com/new-homes/texas/houston</a></p>
</blockquote>
</li>
<li>爬取后的数据ajax抛给指定的接口。</li>
</ol>
<hr>
<p>因为经常会爬一些数据，所以大体看了一下，就一个网址，感觉应该挺简单的。<br>考虑到平时都用py2的scrapy来做爬虫，今天正好再联系下py3，也没有用bs4，因为最近发现lxml的xpath解析挺爽的，<br>直接代码开撸，嘿嘿！</p>
<p>首先，根据要求，需要拿到参数，命令行如下:<br><code>python3 crawl.py https://www.lennar.com/new-homes/washington/seattle</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">if len(argv) != 2:</div><div class="line">	print(&apos;参数格式错误&apos;)</div><div class="line">else:</div><div class="line">	print(argv[1])</div></pre></td></tr></table></figure></p>
<p>开始分析网址、抓取数据，分析网页内容，发现都是动态数据，因此选择用selenium和phantomjs来抓取<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">driver = webdriver.PhantomJS(executable_path=&apos;/Users/apple/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs&apos;)</div><div class="line">driver.get(url)</div><div class="line">selector = etree.HTML(driver.page_source.encode(&apos;utf-8&apos;))</div><div class="line">items = selector.xpath(&apos;//div[@class=&quot;comm-item clearfix&quot;]&apos;)</div><div class="line">	for item in items:</div><div class="line">		temp = item.xpath(&apos;./h1/a/text()&apos;)</div><div class="line">		name = temp[0] if len(temp) &gt; 0 else &apos;&apos;</div><div class="line">		temp = item.xpath(&apos;./h2/a/text()&apos;)</div><div class="line">		name = name + &apos; &apos; + (temp[0] if len(temp) &gt; 0 else &apos;&apos;)</div><div class="line">		print(name)</div></pre></td></tr></table></figure></p>
<p>通过上述代码，很快就抓到首页要的内容，接下来准备抓分页，发现是#标签，然后用chrome来network检查元素，查看post的接口，找到相关的，但post的参数太多，所以想用之前做模拟登陆时的模拟点击来实现，代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">driver.find_element_by_xpath(&apos;//div[@class=&quot;sptop&quot;]//a[@class=&quot;ir next&quot;]&apos;).click()</div></pre></td></tr></table></figure></p>
<p>运行调试，报错了：<br><code>ElementNotVisibleException</code><br>通过报错，然后按以往的经验，加上延时，发现仍然无效，不明所以之后，尝试其他链接的模拟点击，发现可以。最后选择最原始也是最有效的调用js方法来解决。(如果有哪位高人知道原因，可以告诉我，非常感谢)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with open(&apos;test.html&apos;, &apos;w+&apos;) as f:</div><div class="line">	f.write(driver.page_source)</div></pre></td></tr></table></figure></p>
<p>保存下网页，去查看相关的js及变量的内容。通过检查发现调用了两个接口，先做第一个接口，分析参数为页面里js的变量，于是通过js来得到:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">acetParams = driver.execute_script(&quot;return facetContextJSON.params&quot;)</div><div class="line">pageState = driver.execute_script(&quot;return pageState&quot;)</div></pre></td></tr></table></figure></p>
<p>接着通过postman来抓取的header，复制进去，并通过xpath抓取到总页数，通过requests来获取json数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def get_otherpage(page, facetParams, pageState, headers):</div><div class="line">	pageState[&apos;pn&apos;] = page</div><div class="line">	payload = json.dumps(&#123;&apos;searchState&apos;:facetParams, &apos;pageState&apos;: pageState&#125;)</div><div class="line">	url = &apos;https://cn.lennar.com/Services/REST/Facets.svc/GetFacetResults&apos;</div><div class="line">	response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers)</div></pre></td></tr></table></figure></p>
<p>发现成功获取json结果，接着分析第二个接口，参数是第一个接口的返回部分数据，执行代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">url = &quot;https://cn.lennar.com/Services/Rest/SearchMethods.svc/GetCommunityDetails&quot;</div><div class="line">pageState.update(&#123;&quot;pt&quot;:&quot;C&quot;,&quot;ic&quot;:19,&quot;ss&quot;:0,&quot;attr&quot;:&quot;No    ne&quot;,&quot;ius&quot;:False&#125;)</div><div class="line">payload = json.dumps(&#123;&apos;facetResults&apos;: response.json()[&apos;fr&apos;], &apos;pageState&apos;:pageState&#125;)</div><div class="line">response = requests.request(&quot;POST&quot;, url, data=payload, headers=headers)</div><div class="line">for item in response.json():</div><div class="line">	name = item[&apos;cnm&apos;]</div><div class="line">	if item[&apos;mcm&apos;]:</div><div class="line">		name += item[&apos;mcm&apos;]</div><div class="line">	print(name)</div></pre></td></tr></table></figure></p>
<p>到此，所有需求的数据都可以得到了。因为只是测试，所以没有封装成类，也直接使用了postman抓到的cookie.<br>测试代码在<a href="https://github.com/chengu/simple_crawls/blob/master/fang88.py">github</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;接到fang88的技术考核题，要求如下&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用python3&lt;/li&gt;
&lt;li&gt;只写一个crawl.py文件，传递url参数&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;url示例&lt;/strong&gt;：&lt;br&gt;&lt;a
    
    </summary>
    
      <category term="Python" scheme="https://github.com/chengu/chengu.github.io/categories/Python/"/>
    
    
      <category term="爬虫" scheme="https://github.com/chengu/chengu.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
